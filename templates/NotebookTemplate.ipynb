{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template on how to use Pypads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration of environment variables used by pypads\n",
    "Create a .config file under HOME_DIRECTORY/.pypads/ and copy paste the following into that file.\n",
    "\n",
    "    [DEFAULT_PYPADS]\n",
    "    AWS_ACCESS_KEY_ID=xxxxxxxxxx\n",
    "    AWS_SECRET_ACCESS_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    MLFLOW_S3_ENDPOINT_URL=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    MLFLOW_TRACKING_URI=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    MONGO_DB=xxxxx\n",
    "    MONGO_USER=xxxxxxx\n",
    "    MONGO_URL=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "    MONGO_PW=xxxxxxxxxxx\n",
    "    [CONFIG]\n",
    "    mongo_db = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing and initializing PyPads should be the first in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypads.app.base import PyPads\n",
    "tracker = PyPads(autostart='Experiment Name') # This would automatically create the experiment and start a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your imports here, for example\n",
    "import torch\n",
    "import sklearn\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking your Data\n",
    "If you are loading a dataset using a custom function we recommend decorating it (example in the following cell).\n",
    "\n",
    "if you are using a dataset from one of the packages listed above it will be automatically tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracker.decorators.dataset(name=\"Name of your dataset\", target_columns=[], output_format=dict(), metadata=None)\n",
    "def load_data(*args,**kwargs):\n",
    "    \"\"\"\n",
    "    Parameters that can be passed to your decorator\n",
    "    :param name: name of your dataset\n",
    "    :param target_columns: indices/names of targets or labels columns in case the returned dataset is a single object.\n",
    "    :param output_format: A dict describing the outputs of your custom function in case of multiple returned objects.\n",
    "    :param metadata: a dict holding extra meta information on your dataset (kind of features, source, description, etc..)\n",
    "\n",
    "    Example:\n",
    "            def load_data():\n",
    "                X, y = make_classification(n_samples=150)\n",
    "                return X,y\n",
    "\n",
    "            For this function we have to give the output_format as follows:\n",
    "                output_format = {'X': 'features', 'y': 'targets'} such that entries of the dict have the same order\n",
    "                as the outputs. Keys of the dict would dictate the names of the stored binaries:\n",
    "                    - Generated_Dataset_X.pickle, Generated_Dataset_y.pickle\n",
    "\n",
    "            @tracker.decorators.dataset(name=\"Generated Dataset\", output_format={'X': 'features', 'y': 'targets'})\n",
    "            def load_data():\n",
    "                X, y = make_classification(n_samples=150)\n",
    "                return X,y\n",
    "    \"\"\"\n",
    "    # Do your loading here.\n",
    "\n",
    "    # Supported formats of outputs are: numpy arrays, dataframes, tuples of such types.\n",
    "    # In case your data is defined on more than one object then try to describe the output\n",
    "    X,y = \"X\",\"y\"\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do your preprocessing on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When defining your model/experiment's hyperparameters try to log them manually. As an example:\n",
    "number_of_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# We log them as following\n",
    "# tracker.api.log_param(key, value, value_format=None, description=\"\", additional_data: dict = None)\n",
    "tracker.api.log_param(\"number_of_epochs\", 100, description=\"Number of training epochs\")\n",
    "tracker.api.log_param(\"batch_size\", 50, description=\"Size of training/testing batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Torch model defined using a custom class\n",
    "In this case, you can use a decorator defined in pypads.\n",
    "    \n",
    "```python\n",
    "@tracker.decorators.watch(track=\"hyper-parameters\", debugging=False)\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # define your layers as attributes.\n",
    "        self.InputLayer = ...\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your forward function\n",
    "        return out\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating of your model\n",
    "When evaluating your model whether iteratively when training, or as a single time when testing. Try to log your metrics values manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example would be like the following:\n",
    "from sklearn.metrics.classification import f1_score\n",
    "score = f1_score(predicted, truth, average=\"macro\")\n",
    "tracker.api.log_metric('f1_score', score, description=\"F1 score evaluation of the model\", step=0)\n",
    "\n",
    "# For an iterative logging:\n",
    "for epoch in range(number_of_epochs):\n",
    "    # Train or test\n",
    "    # Log your metric, loss, etc..\n",
    "    tracker.api.log_metric('training_loss', loss, description=\"Training Loss of the model\", step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  End the run of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.api.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
